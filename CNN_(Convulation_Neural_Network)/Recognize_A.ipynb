{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b26df2c7-ffa5-4e1b-b5c2-6331ce47eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.label_mapping = {}\n",
    "        current_label = 0\n",
    "\n",
    "        # Valid image extensions\n",
    "        valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif')\n",
    "\n",
    "        for label in os.listdir(root_dir):\n",
    "            label_path = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                self.label_mapping[label] = current_label\n",
    "                current_label += 1\n",
    "                for img_name in os.listdir(label_path):\n",
    "                    img_path = os.path.join(label_path, img_name)\n",
    "                    if os.path.isfile(img_path) and img_path.endswith(valid_extensions):\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(self.label_mapping[label])\n",
    "\n",
    "        # Debugging output\n",
    "        print(f\"Loaded {len(self.image_paths)} images with labels {set(self.labels)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert image to grayscale\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a8dd70f-81c4-4130-914c-d94d6d57275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "885a02ef-0b51-41ec-8062-e16857b0da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 126, 126])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming `model` is defined elsewhere\n",
    "sample_input = torch.randn(1, 1, 256, 256)  # Single grayscale image, 256x256\n",
    "conv_output = model.conv1(sample_input)\n",
    "conv_output = model.conv2(conv_output)\n",
    "conv_output = nn.functional.max_pool2d(conv_output, 2)\n",
    "print(conv_output.shape)  # Check output shape for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eb8b896-d033-46c6-b7a7-95d1bd279342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 images with labels set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[0;32m     35\u001b[0m dataset \u001b[38;5;241m=\u001b[39m HandwritingDataset(root_dir\u001b[38;5;241m=\u001b[39mroot_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m---> 36\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Debugging: Check if DataLoader works correctly\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# for images, labels in train_loader:\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#     # Apply label mapping\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#     labels = binary_label_mapping(labels)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#     print(images.shape, labels)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     break  # Remove this break after debugging\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your HandwritingDataset and DataLoader\n",
    "root_dir = \"./A\"  # Adjust this if the \"A\" folder is in a different location\n",
    "\n",
    "# Define a function to load only image files\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "\n",
    "# Adjust HandwritingDataset to ignore non-image files\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Filter to include only image files\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if is_image_file(f)]\n",
    "        print(f\"Loaded {len(self.image_files)} images with labels set\")  # Debugging output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"L\")  # Convert to grayscale if needed\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = 1  # Assign label based on your specific setup\n",
    "        return image, label\n",
    "\n",
    "# Map labels {1, 2} -> {1, 0}\n",
    "def binary_label_mapping(labels):\n",
    "    return torch.where(labels == 2, torch.tensor(0), labels)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = HandwritingDataset(root_dir=root_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Debugging: Check if DataLoader works correctly\n",
    "# for images, labels in train_loader:\n",
    "#     # Apply label mapping\n",
    "#     labels = binary_label_mapping(labels)\n",
    "#     print(images.shape, labels)\n",
    "#     break  # Remove this break after debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9b9783f-67ca-4f12-8b75-a79c0d0fddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32 * 30 * 30, 64)  # Adjust based on the actual output shape\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        \n",
    "        # Check the shape of x dynamically\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4b63deb-a363-4023-a443-b4e1f33f5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, criterion, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f2a2afc-a828-49f7-b3fd-b6cfa4a5e868",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Ensure images are transformed and labels are converted\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Ensure images are transformed and labels are converted\n",
    "        labels = torch.tensor([1 if label == 'A' else 0 for label in labels]).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd54464c-8d5a-40f9-8b6f-d54e4bcb363e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \u001b[38;5;66;03m# For multi-class classification\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[0;32m     13\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     14\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()  \u001b[38;5;66;03m# Apply threshold 0.5 for binary classification\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images\n",
    "#         outputs = model(images)\n",
    "#         predictions = (outputs.squeeze() > 0.5).int()  # Threshold at 0.5\n",
    "#         print(f\"Predicted: {predictions}, Actual: {labels}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        predictions = (outputs.squeeze() > 0.5).int()  # Apply threshold 0.5 for binary classification\n",
    "        print(f\"Predicted: {predictions}, Actual: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487f8d39-f759-4cbc-a8a2-0ed7750fa60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Character_A.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "808860bc-b9b7-41de-baed-517bb8d3da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: A\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load an image and apply transformations\n",
    "img_path = 'C:/Users/sansk/Desktop/our_project_volume_II/CNN_(Convulation_Neural_Network)/A/2.jpg'  # Replace with your test image path\n",
    "img = Image.open(img_path).convert('L')\n",
    "img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img)  # Model output for a single image\n",
    "    prediction = output.item()  # Get scalar prediction\n",
    "\n",
    "    # Apply threshold\n",
    "    if prediction >= 0.5:\n",
    "        print(\"Predicted: A\")\n",
    "    else:\n",
    "        print(\"Predicted: Not A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a048ba4-e744-4d6d-b2bc-4c4f07636ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd260f-03ae-4db6-9687-a47d0a6e2aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c919de80-a779-4c85-843c-d2dfb856177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Not A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to the image\n",
    "img_path = 'C:/Users/sansk/Desktop/our_project_volume_II/CNN_(Convulation_Neural_Network)/test_data/img5.jpg'\n",
    "img = Image.open(img_path).convert('L')\n",
    "img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img)  # Output shape: [1, 1] for a single image\n",
    "    prediction = output.item()  # Get scalar prediction\n",
    "\n",
    "    # Decide based on a threshold (e.g., 0.5 for binary classification)\n",
    "    if prediction >= 0.7:\n",
    "        print(\"Predicted: A\")\n",
    "    else:\n",
    "        print(\"Predicted: Not A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20e6cc-6249-4b21-8e05-e519e5851b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb09783-ae6b-4db0-9b3f-95d7babdd5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
